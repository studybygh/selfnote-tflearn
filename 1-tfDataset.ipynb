{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cec5fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0c48ffc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2013 2013] 12000\n",
      "[2014 2014] 14000\n",
      "[2015 2015] 15000\n",
      "[2016 2016] 16500\n",
      "[2017 2017] 17500\n",
      "[{'a': (1, 3), 'b': 5}, {'a': (2, 4), 'b': 6}]\n",
      "(1, 3)\n",
      "6\n",
      "(array([[1, 3],\n",
      "       [2, 3]]), array([[b'A'],\n",
      "       [b'A']], dtype=object))\n",
      "(array([[2, 1],\n",
      "       [1, 2]]), array([[b'B'],\n",
      "       [b'B']], dtype=object))\n",
      "(array([[3, 3],\n",
      "       [3, 2]]), array([[b'A'],\n",
      "       [b'B']], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[2013, 2013], [2014,2014], [2015,2015], [2016,2016], [2017,2017]])\n",
    "Y = tf.constant([12000, 14000, 15000, 16500, 17500])\n",
    "# 也可以使用NumPy数组，效果相同\n",
    "# X = np.array([2013, 2014, 2015, 2016, 2017])\n",
    "# Y = np.array([12000, 14000, 15000, 16500, 17500])\n",
    "# 当提供多个张量作为输入时，张量的第 0 维大小必须相同，且必须将多个张量作为元组 (Tuple，即使用 Python 中的小括号) 拼接并作为输入。\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "for x, y in dataset:\n",
    "    print(x.numpy(), y.numpy()) \n",
    "\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),'b': [5, 6]})\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "print(list(dataset.as_numpy_iterator())[0]['a'])\n",
    "print(list(dataset.as_numpy_iterator())[1]['b'])\n",
    "\n",
    "\n",
    "# Two tensors can be combined into one Dataset object.\n",
    "features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
    "labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "# Both the features and the labels tensors can be converted\n",
    "# to a Dataset object separately and combined after.\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n",
    "# A batched feature and label set can be converted to a Dataset\n",
    "# in similar fashion.\n",
    "batched_features = tf.constant([[[1, 3], [2, 3]],\n",
    "                                [[2, 1], [1, 2]],\n",
    "                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
    "batched_labels = tf.constant([['A', 'A'],\n",
    "                              ['B', 'B'],\n",
    "                              ['A', 'B']], shape=(3, 2, 1))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((batched_features, batched_labels))\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4fb5a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'file1\\t1', shape=(), dtype=string)\n",
      "tf.Tensor(b'file1\\t2', shape=(), dtype=string)\n",
      "------------------------\n",
      "------------------------\n",
      "tf.Tensor(b'tfDatasetFile\\\\file1.txt', shape=(), dtype=string)\n",
      "tf.Tensor(b'tfDatasetFile\\\\file2.txt', shape=(), dtype=string)\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TextLineDataset([\"tfDatasetFile/file1.txt\", \"tfDatasetFile/file2.txt\"])\n",
    "for element in dataset:\n",
    "    print(element)\n",
    "print('------------------------')\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"tfDatasetFile/file1.tfrecords\", \"tfDatasetFile/file2.tfrecords\"])\n",
    "for element in dataset:\n",
    "    print(element)\n",
    "print('------------------------')\n",
    "   \n",
    "    \n",
    "dataset = tf.data.Dataset.list_files(\"tfDatasetFile/*.txt\")  # doctest: +SKIP\n",
    "for element in dataset:\n",
    "    print(element)\n",
    "print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "802fde9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flat_map(\n",
    "#     map_func\n",
    "# )\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "list(dataset.as_numpy_iterator())\n",
    "dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6f8d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
      "[2, 4, 6]\n",
      "[1, 2, 3]\n",
      "[b'Tensor(\"args_0:0\", dtype=int32)foo', b'Tensor(\"args_0:0\", dtype=int32)bar', b'Tensor(\"args_0:0\", dtype=int32)baz']\n",
      "[b'HELLO', b'WORLD']\n",
      "[b'HELLO', b'WORLD']\n"
     ]
    }
   ],
   "source": [
    "# map(\n",
    "#     map_func, num_parallel_calls=None, deterministic=None\n",
    "# )\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "print(dataset.element_spec)\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "# Each element is a tuple containing two `tf.Tensor` objects.\n",
    "elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: elements, (tf.int32, tf.string))\n",
    "# `map_func` takes two arguments of type `tf.Tensor`. This function\n",
    "# projects out just the first component.\n",
    "result = dataset.map(lambda x_int, y_str: x_int)\n",
    "print(list(result.as_numpy_iterator()))\n",
    "\n",
    "# Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
    "elements =  ([{\"a\": 1, \"b\": \"foo\"},\n",
    "              {\"a\": 2, \"b\": \"bar\"},\n",
    "              {\"a\": 3, \"b\": \"baz\"}])\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n",
    "# `map_func` takes a single argument of type `dict` with the same keys\n",
    "# as the elements.\n",
    "result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n",
    "print(list(result.as_numpy_iterator()))\n",
    "\n",
    "\n",
    "# Note that tf.py_function accepts tf.Tensor \n",
    "# whereas tf.numpy_function accepts numpy arrays and returns only numpy arrays\n",
    "d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
    "# transform a string tensor to upper case string using a Python function\n",
    "def upper_case_fn(t: tf.Tensor):\n",
    "    return t.numpy().decode('utf-8').upper()\n",
    "d = d.map(lambda x: tf.py_function(func=upper_case_fn, inp=[x], Tout=tf.string))\n",
    "print(list(d.as_numpy_iterator()))\n",
    "\n",
    "d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
    "def upper_case_fn(t: np.ndarray):\n",
    "    return t.decode('utf-8').upper()\n",
    "d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,inp=[x], Tout=tf.string))\n",
    "print(list(d.as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8e2fc260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# apply(\n",
    "#     transformation_func\n",
    "# ) \n",
    "# transformation_func\tA function that takes one Dataset argument and returns a Dataset.\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "def dataset_fn(ds):\n",
    "    return ds.filter(lambda x: x < 5)\n",
    "def dataset_tn(ds):\n",
    "    print(list(ds.as_numpy_iterator()))\n",
    "    return ds\n",
    "dataset = dataset.apply(dataset_fn)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "\n",
    "dataset = dataset.apply(dataset_tn)\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3bbe166e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2], dtype=int64), array([3, 4, 5], dtype=int64)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch(\n",
    "#     batch_size, drop_remainder=False\n",
    "# )\n",
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.batch(3, drop_remainder=True)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c3dcc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "[(1, 4), (2, 5), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "# concatenate(\n",
    "#     dataset\n",
    "# )\n",
    "\n",
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = a.concatenate(b)\n",
    "print(list(ds.as_numpy_iterator()))\n",
    "\n",
    "# The input dataset and dataset to be concatenated should have the same\n",
    "# nested structures and output types.\n",
    "c = tf.data.Dataset.zip((a, b))\n",
    "print(list(c.as_numpy_iterator()))\n",
    "# a.concatenate(c)   Two datasets to concatenate have different types <dtype: 'int64'> and (tf.int64, tf.int64)\n",
    "\n",
    "\n",
    "d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
    "# a.concatenate(d) # Two datasets to concatenate have different types <dtype: 'int64'> and <dtype: 'string'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a7cd0476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# filter(\n",
    "#     predicate\n",
    "# )\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.filter(lambda x: x < 3)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "# `tf.math.equal(x, y)` is required for equality comparison\n",
    "def filter_fn(x):\n",
    "    return tf.math.equal(x, 1)\n",
    "dataset = dataset.filter(filter_fn)\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "db2a9033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interleave(\n",
    "#     map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n",
    "#     deterministic=None\n",
    "# )\n",
    "# The cycle_length and block_length arguments control the order in which elements are produced. \n",
    "# cycle_length controls the number of input elements that are processed concurrently. \n",
    "# If you set cycle_length to 1, this transformation will handle one input element at a time, \n",
    "# and will produce identical results to tf.data.Dataset.flat_map.\n",
    "\n",
    "dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "# NOTE: New lines indicate \"block\" boundaries.\n",
    "dataset = dataset.interleave(lambda x: tf.data.Dataset.from_tensors(x).repeat(4), cycle_length=2, block_length=4)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2f646d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DatasetV2.as_numpy_iterator of <MapDataset shapes: (None,), types: tf.int32>>\n",
      "-------------------------------\n",
      "[[1 0]\n",
      " [2 2]]\n",
      "[[3 3 3 0]\n",
      " [4 4 4 4]]\n",
      "-------------------------------\n",
      "[[1 0 0 0 0]\n",
      " [2 2 0 0 0]]\n",
      "[[3 3 3 0 0]\n",
      " [4 4 4 4 0]]\n",
      "-------------------------------\n",
      "[[ 1 -1 -1 -1 -1]\n",
      " [ 2  2 -1 -1 -1]]\n",
      "[[ 3  3  3 -1 -1]\n",
      " [ 4  4  4  4 -1]]\n",
      "-------------------------------\n",
      "[(array([1, 2, 3]), array([10])), (array([4, 5]), array([11, 12]))]\n",
      "[(array([[ 1,  2,  3, -1],\n",
      "       [ 4,  5, -1, -1]]), array([[ 10, 100],\n",
      "       [ 11,  12]]))]\n",
      "[(array([[ 1,  2,  3, -1],\n",
      "       [ 4,  5, -1, -1]]), array([[ 10, 100, 100],\n",
      "       [ 11,  12, 100]]))]\n",
      "-------------------------------\n",
      "(array([[ 1, -1],\n",
      "       [ 2,  2]]), array([[ 1, -1],\n",
      "       [ 2,  2]]))\n",
      "(array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]]), array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]]))\n"
     ]
    }
   ],
   "source": [
    "# padded_batch(\n",
    "#     batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n",
    "# )\n",
    "A = (tf.data.Dataset.range(1, 5, output_type=tf.int32).map(lambda x: tf.fill([x], x)))\n",
    "print(A.as_numpy_iterator)\n",
    "print('-------------------------------')\n",
    "\n",
    "\n",
    "# Pad to the smallest per-batch size that fits all elements.\n",
    "B = A.padded_batch(2)\n",
    "for element in B.as_numpy_iterator():\n",
    "    print(element)\n",
    "print('-------------------------------')\n",
    "\n",
    "# Pad to a fixed size.\n",
    "C = A.padded_batch(2, padded_shapes=5)\n",
    "for element in C.as_numpy_iterator():\n",
    "    print(element)\n",
    "print('-------------------------------')\n",
    "    \n",
    "# Pad with a custom value.\n",
    "D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
    "for element in D.as_numpy_iterator():\n",
    "    print(element)\n",
    "print('-------------------------------')\n",
    "    \n",
    "# Components of nested elements can be padded independently.\n",
    "elements = [([1, 2, 3], [10]),\n",
    "            ([4, 5], [11, 12])]\n",
    "datasetc = tf.data.Dataset.from_generator(lambda: iter(elements), (tf.int32, tf.int32))\n",
    "print(list(datasetc.as_numpy_iterator()))\n",
    "# Pad the first component of the tuple to length 4, and the second\n",
    "# component to the smallest size that fits.\n",
    "dataset = datasetc.padded_batch(2,\n",
    "    padded_shapes=([4], [None]),\n",
    "    padding_values=(-1, 100))\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = datasetc.padded_batch(2,\n",
    "    padded_shapes=([4], [3]),\n",
    "    padding_values=(-1, 100))\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "print('-------------------------------')\n",
    "\n",
    "# Pad with a single value and multiple components.\n",
    "E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
    "for element in E.as_numpy_iterator():\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "28f14284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# reduce(\n",
    "#     initial_state, reduce_func\n",
    "# )\n",
    "# A reduce_func that maps (old_state, input_element) to new_state. \n",
    "# It must take two arguments and return a new element\n",
    "# The structure of new_state must match the structure of initial_state.\n",
    "print(tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy())\n",
    "print(tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d01c87f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1, 0, 1, 2]\n",
      "[1, 2, 0, 1, 2, 0]\n",
      "[2, 1, 0]\n",
      "[2, 0, 1]\n",
      "[2, 1, 0]\n",
      "[2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# shuffle(\n",
    "#     buffer_size, seed=None, reshuffle_each_iteration=None\n",
    "# )\n",
    "# reshuffle_each_iteration controls whether the shuffle order should be different for each epoch\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
    "dataset = dataset.repeat(2)  # doctest: +SKIP\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
    "dataset = dataset.repeat(2)  # doctest: +SKIP\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
    "print(list(dataset.as_numpy_iterator()))  # doctest: +SKIP\n",
    "print(list(dataset.as_numpy_iterator()))  # doctest: +SKIP\n",
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
    "print(list(dataset.as_numpy_iterator()))  # doctest: +SKIP\n",
    "print(list(dataset.as_numpy_iterator()))  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f58f24a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# skip(\n",
    "#     count\n",
    "# )\n",
    "# Creates a Dataset that skips count elements from this dataset.\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.skip(7)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "\n",
    "# take(\n",
    "#     count\n",
    "# )\n",
    "# Creates a Dataset with at most count elements from this dataset.\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.take(3)\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "44c2f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3], dtype=int64), array([1, 2], dtype=int64), array([1, 2, 3, 4], dtype=int64)]\n",
      "[1, 2, 3, 1, 2, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# unbatch() Splits elements of a dataset into multiple elements.\n",
    "elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\n",
    "dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "dataset = dataset.unbatch()\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "58814cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n",
      "[4, 5]\n",
      "[6]\n",
      "[0, 1, 2]\n",
      "[2, 3, 4]\n",
      "[4, 5, 6]\n",
      "-------------------------------\n",
      "[0, 2, 4]\n",
      "[1, 3, 5]\n",
      "[2, 4, 6]\n",
      "-------------------------------\n",
      "([1, 2], [5, 6])\n",
      "([3, 4], [7, 8])\n",
      "-------------------------------\n",
      "{'a': [1, 2]}\n",
      "{'a': [3, 4]}\n"
     ]
    }
   ],
   "source": [
    "# window(\n",
    "#     size, shift=None, stride=1, drop_remainder=False\n",
    "# )\n",
    "dataset = tf.data.Dataset.range(7).window(2)\n",
    "for window in dataset:\n",
    "    print(list(window.as_numpy_iterator()))\n",
    "\n",
    "dataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\n",
    "for window in dataset:\n",
    "    print(list(window.as_numpy_iterator()))\n",
    "print('-------------------------------')\n",
    "    \n",
    "dataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\n",
    "for window in dataset:\n",
    "    print(list(window.as_numpy_iterator()))\n",
    "print('-------------------------------')\n",
    "    \n",
    "nested = ([1, 2, 3, 4], [5, 6, 7, 8])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\n",
    "for window in dataset:\n",
    "    def to_numpy(ds):\n",
    "        return list(ds.as_numpy_iterator())\n",
    "    print(tuple(to_numpy(component) for component in window))\n",
    "print('-------------------------------')\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\n",
    "dataset = dataset.window(2)\n",
    "for window in dataset:\n",
    "    def to_numpy(ds):\n",
    "        return list(ds.as_numpy_iterator())\n",
    "    print({'a': to_numpy(window['a'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ffaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
